ä¸‹â¾¯ç»™å‡ºä¸€å¥— **ç«¯åˆ°ç«¯ Python æµç¨‹**ï¼ŒæŠŠä½ å·²ç»å¾—åˆ°çš„

```
ğŸ“„ English.txt   +   ğŸ“„ ä¸­æ–‡.txt
```

åˆ‡æˆã€Œ**â‰ˆ8 k æ±‰å­—**ã€çš„æˆæ®µå¯¹ï¼Œå¹¶è¾“å‡ºä¸ºå¯ç›´æ¥å–‚è¿›å¤§æ¨¡å‹å¾®è°ƒçš„ JSONL æ•°æ®é›†ã€‚
ä»£ç æŒ‰ **æ¸…æ´— â†’ å¥çº§å¯¹é½ â†’ 8 k æ‹¼æ®µ â†’ æ ¼å¼åŒ–è¾“å‡º** å››æ­¥ç»„ç»‡ï¼›æ‰€æœ‰ç­–ç•¥éƒ½ä¸ Seek-Hub æŒ‡å— *4/5 èŠ‚* è¦æ±‚ä¿æŒä¸€è‡´ï¼ˆå¥â»“å¯¹é½ + åƒåœ¾è¡Œæ¸…é™¤ + UTF-8 ä¿å­˜ï¼‰ã€‚

---

## ç¯å¢ƒä¾èµ–

```bash
# å¥å­åµŒâ¼Š + å¯¹é½
pip install -U sentence-transformers torch bertalign tqdm

# ä¸­æ–‡æ–­å¥ + æ¸…æ´—å¯é€‰
pip install opencc-python-reimplemented regex

# NLTK è‹±æ–‡æ–­å¥
pip install nltk
python -m nltk.downloader punkt
```

* **LaBSE** å¤šè¯­å¥å‘é‡æ¨¡å‹å¯æ˜ å°„ 109 è¯­ç§åˆ°åŒä¸€ç©ºé—´ï¼Œé€‚åˆæ‰¾ç¿»è¯‘å¯¹ ([Hugging Face][1], [SentenceTransformers][2])
* **Bertalign** å°è£…äº†ã€Œå¥å‘é‡ + åŠ¨è§„ã€ç®—æ³•ï¼Œèƒ½è¾“å‡ºé¡ºåºä¸€è‡´çš„å¥å¯¹ ([GitHub][3], [arXiv][4])
* è‹¥æƒ³ç”¨ç»å…¸é•¿åº¦/è¯å…¸å¯¹é½ï¼Œä¹Ÿå¯æ¢ **Hunalign** ([GitHub][5], [Mokk][6], [Nessie][7])

---

## 1 . æ–‡æœ¬é¢„æ¸…æ´—ï¼ˆå¯é€‰ï¼‰

```python
import regex as re
from opencc import OpenCC               # è‹¥éœ€ç¹â†’ç®€
cc = OpenCC("t2s")

def clean_txt(raw: str, lang="zh") -> str:
    """ç²—æ¸…æ´—ï¼šå» BOM / æ§åˆ¶ç¬¦ / å¤šç©ºè¡Œ / ç®€ç¹è½¬æ¢"""
    txt = raw.replace("\ufeff", "").replace("\r\n", "\n")          # BOM / CRLF
    txt = re.sub(r"[\x00-\x09\x0B-\x1F\x7F]", "", txt)             # æ§åˆ¶ç¬¦
    txt = re.sub(r"\n{3,}", "\n\n", txt)                           # æŠ˜å ç©ºè¡Œ
    if lang == "zh":
        txt = cc.convert(txt)                                      # ç¹â†’ç®€
    return txt.strip()
```

> Seek-Hub ç¬¬ 5 èŠ‚è¦æ±‚ã€Œç¼–ç ç»Ÿä¸€ / å»æ§åˆ¶ç¬¦ / æŠ˜å ç©ºè¡Œã€â€”â€”è¿™é‡Œä¸€æ¬¡æ€§å®Œæˆã€‚
> OpenCC æ”¯æ’‘ç¹ç®€è½¬æ¢ ([GitHub][8], [GitHub][9])

---

## 2 . å¥çº§å¯¹é½ï¼ˆLaBSE + Bertalignï¼‰

```python
from bertalign.aligner import Aligner
import nltk, regex
nltk.download("punkt")
aligner = Aligner(model_name="sentence-transformers/LaBSE")  # ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹

def split_en(text):                 # NLTK è‹±æ–‡åˆ†å¥
    return nltk.sent_tokenize(text)

def split_zh(text):                 # æ­£åˆ™ä¸­æ–‡åˆ†å¥
    return [s for s in regex.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', text) if s.strip()]

def align_sentences(en_sents, zh_sents):
    """
    è¿”å›æŒ‰é¡ºåºå¯¹åº”çš„ [(en_idx, zh_idx)] åˆ—è¡¨ï¼›Bertalign å†…éƒ¨å·²åšåŠ¨æ€è§„åˆ’
    """
    pairs = aligner.align(en_sents, zh_sents, threshold=0.5)
    return pairs
```

LaBSE åœ¨å¥å¯¹æŠ½å–ä»»åŠ¡ä¸Šçš„æ•ˆæœå·²è¢«å¤šç¯‡ç ”ç©¶è¯å® ([arXiv][4], [aclanthology.org][10])ã€‚

---

## 3 . æŒ‰ 8 k æ±‰å­—æ‹¼æ®µ

```python
import json, math, uuid
from pathlib import Path
from tqdm import tqdm

CHUNK_SIZE = 8_000        # 8k æ±‰å­—

def build_dataset(en_path, zh_path, out_jsonl):
    en_txt = clean_txt(Path(en_path).read_text(), lang="en")
    zh_txt = clean_txt(Path(zh_path).read_text(), lang="zh")

    en_sents = split_en(en_txt)
    zh_sents = split_zh(zh_txt)

    pairs = align_sentences(en_sents, zh_sents)

    # â€”â€” æŒ‰ 8k æ±‰å­—èšåˆ â€”â€” #
    buf_en, buf_zh, buf_len = [], [], 0
    with open(out_jsonl, "w", encoding="utf-8") as fout:
        for en_idx, zh_idx in tqdm(pairs, desc="æ‹¼æ®µ"):
            esent, zsent = en_sents[en_idx], zh_sents[zh_idx]
            buf_en.append(esent)
            buf_zh.append(zsent)
            buf_len += len(zsent)

            if buf_len >= CHUNK_SIZE:
                record = {
                    "id": str(uuid.uuid4()),
                    "english": " ".join(buf_en),
                    "chinese": "".join(buf_zh)
                }
                fout.write(json.dumps(record, ensure_ascii=False) + "\n")
                buf_en, buf_zh, buf_len = [], [], 0   # reset

        # æ”¶å°¾ä¸è¶³ 8k çš„æ®‹æ®µï¼ˆå¯é€‰ä¸¢å¼ƒï¼‰
        if buf_en and buf_len > CHUNK_SIZE * 0.3:     # è‡³å°‘ 30% é•¿åº¦
            record = {
                "id": str(uuid.uuid4()),
                "english": " ".join(buf_en),
                "chinese": "".join(buf_zh)
            }
            fout.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"âœ“ å®Œæˆï¼šè¾“å‡ºè‡³ {out_jsonl}")
```

* æ®µé•¿ä»¥ã€Œä¸­æ–‡å­—æ•°ã€è®¡ï¼›æŒ‰ Seek-Hub 4 èŠ‚å»ºè®®ï¼Œå‰©ä½™ä¸è¶³ 30 % çš„æ®‹ç‰‡å¯ä¸¢å¼ƒæˆ–å¹¶å…¥ä¸Šä¸€æ®µã€‚
* è¾“å‡ºæ¯è¡Œä¸€æ¡ JSONï¼ˆ`id/en/chinese`ï¼‰ï¼Œæ–¹ä¾¿æµå¼è¯»å–ã€‚

---

## 4 . æ‰¹é‡å¤„ç†å¤šæœ¬ä¹¦

```python
from glob import glob
for zh_file in glob("txt_clean/*_zh.txt"):      # ä½ è‡ªå·±çš„å‘½åè§„åˆ™
    en_file = zh_file.replace("_zh.txt", "_en.txt")
    out_file = zh_file.replace("_zh.txt", ".jsonl")
    build_dataset(en_file, zh_file, out_file)
```

---

## 5 . ç»“æœæ£€æŸ¥ä¸è¿›é˜¶

| æ£€æŸ¥ç‚¹    | åšæ³•                                                                |
| ------ | ----------------------------------------------------------------- |
| å¯¹é½è¯¯å·®   | éšæœºæŠ½ 20 æ¡ï¼Œæ ¸å¯¹é¦–æœ«å¥å«ä¹‰ï¼›è¯¯å·®å¤š â†’ è°ƒé«˜ `threshold` æˆ–ç”¨è¯å…¸+Hunalign ([GitHub][5]) |
| ç©ºç™½/ä¹±ç   | grep æˆ–è„šæœ¬ç»Ÿè®¡ `len(en)==0 or len(zh)==0` è¡Œæ•°                          |
| é•¿åº¦åˆ†å¸ƒ   | ç»Ÿè®¡ `len(chinese)` çš„å‡å€¼/æ–¹å·®ï¼Œç¡®ä¿é›†ä¸­åœ¨ 8 kÂ±10 %                           |
| æ¨¡å‹æ¨ç†éªŒè¯ | ç”¨ç°æˆ MT è¯„ä¼°æ¨¡å‹æ‰“ BLEU/LASER ç›¸ä¼¼åº¦ï¼Œä½åˆ†æ¡å¤æŸ¥                                 |

å¦‚éœ€æ›´å¼ºåˆ¶å¯¹é½ï¼ŒHunalignï¼ˆé•¿åº¦+è¯å…¸ï¼‰æˆ– Vecalignã€SimAlign éƒ½èƒ½ç›´æ¥æ¢è¿›æ¥è€Œä¸å½±å“åç»­æ‹¼æ®µ ([Mokk][6], [GitHub][3])ã€‚

---

## å®Œæ•´å·¥ä½œæµå›é¡¾

1. **æ¸…æ´—**ï¼šå»æ§åˆ¶ç¬¦ã€æŠ˜å ç©ºè¡Œã€ç¹â†’ç®€ï¼›ä¿è¯ UTF-8ã€‚
2. **å¥çº§å¯¹é½**ï¼šLaBSE åµŒâ¼Š + Bertalign åŠ¨è§„ï¼Œè¾“å‡ºé¡ºåºä¸€è‡´å¥å¯¹ã€‚
3. **8 k æ‹¼æ®µ**ï¼šç´¯åŠ ä¸­æ–‡å­—æ•°åˆ°é˜ˆå€¼ï¼Œç”Ÿæˆ `(english, chinese)` å¤§ç‰‡æ®µã€‚
4. **æ ¼å¼åŒ–è¾“å‡º**ï¼šé€è¡Œ JSONLï¼Œå­—æ®µ `id/english/chinese`ï¼›å¯ç›´æ¥ `jq` / `datasets` è¯»å–ã€‚

æŒ‰æ­¤ä»£ç å³å¯è‡ªåŠ¨æŠŠå¤šæœ¬ä¸­è‹±è¯‘æœ¬æ‰¹é‡åˆ‡å—ï¼Œå¯¹é½ç”Ÿæˆå¯å¾®è°ƒçš„æ•°æ®é›†ã€‚è‹¥éœ€è¿›ä¸€æ­¥ç»†åŒ–ï¼ˆå¦‚ç« çº§æ ‡é¢˜ä¿ç•™ã€ç‰¹æ®Šç¬¦å·æ ‡å‡†åŒ–ï¼‰ï¼Œåªè¦åœ¨ *clean\_txt()* æˆ– *build\_dataset()* é‡Œæ’ç›¸åº”é€»è¾‘å³å¯ã€‚ç¥æ„å»ºé¡ºåˆ©!

[1]: https://huggingface.co/sentence-transformers/LaBSE?utm_source=chatgpt.com "sentence-transformers/LaBSE - Hugging Face"
[2]: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html?utm_source=chatgpt.com "Pretrained Models â€” Sentence Transformers documentation"
[3]: https://github.com/bfsujason/bertalign?utm_source=chatgpt.com "bfsujason/bertalign: Multilingual sentence alignment using ... - GitHub"
[4]: https://arxiv.org/abs/2301.12140?utm_source=chatgpt.com "Multilingual Sentence Transformer as A Multilingual Word Aligner"
[5]: https://github.com/danielvarga/hunalign?utm_source=chatgpt.com "danielvarga/hunalign: Sentence aligner - GitHub"
[6]: https://mokk.bme.hu/en/resources/hunalign/?utm_source=chatgpt.com "hunalign â€“ sentence aligner | MÃ©dia OktatÃ³ Ã©s KutatÃ³ KÃ¶zpont - BME"
[7]: https://nessie.ilab.sztaki.hu/~kornai/Hunglish/src/hunalign/readme.html?utm_source=chatgpt.com "Hunalign"
[8]: https://github.com/BYVoid/OpenCC?utm_source=chatgpt.com "BYVoid/OpenCC: Conversion between Traditional and ... - GitHub"
[9]: https://github.com/yichen0831/opencc-python?utm_source=chatgpt.com "yichen0831/opencc-python - GitHub"
[10]: https://aclanthology.org/2022.findings-emnlp.215.pdf?utm_source=chatgpt.com "[PDF] Multilingual Sentence Transformer as A Multilingual Word Aligner"
