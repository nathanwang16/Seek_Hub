下面给出一份**可直接运行的 Python 脚本示例**，实现以下目标：

1. **判断 PDF 是“文字层”还是纯扫描页”**
2. **优先用 PyMuPDF 提取文字层**；若某页没有文字，则**自动调用 Tesseract OCR**（可同时识别中英文）
3. 对提取到的文本做 **基础清洗**：去除分页号/页眉页脚、合并断行、统一编码
4. 将整本书输出为 UTF‑8 编码的 `.txt` 文件
5. 给出 **EPUB→TXT** 的两种做法（调用 Calibre 或纯 Python 解析）

---

## 0 . 环境准备

```bash
# 系统需先安装 Tesseract 主程序 (Linux: sudo apt-get install tesseract-ocr)
# 并安装中文简体和英文语言包:
#   sudo apt-get install tesseract-ocr-chi-sim tesseract-ocr-eng

# Python 依赖
pip install PyMuPDF pillow pytesseract ebooklib
# （若采用 Calibre / Pandoc 方案，再把它们装到系统即可）
```

---

## 1 . PDF ➜ TXT 脚本

```python
import re
import os
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
from tempfile import mkstemp
from pathlib import Path

# ----------  参数 ----------
TESSERACT_LANG = "eng+chi_sim"      # 同时识别英语+简体中文
OCR_DPI        = 300                # 渲染图片分辨率，数值越高 OCR 越准确
MAX_IMG_MB     = 10                 # 单张渲染图片大小上限 (MB) 防止内存炸

# ----------  核心函数 ----------
def pdf_to_txt(pdf_path: str, txt_path: str) -> None:
    """Extract text from a PDF (mixed text & scanned). Save UTF‑8 txt."""
    doc = fitz.open(pdf_path)
    out_lines = []

    for page_num, page in enumerate(doc, 1):
        text = page.get_text("text")
        if text.strip():                     # 该页本身有文字
            out_lines.append(text)
        else:                                # OCR fallback
            img_text = ocr_page(page)
            out_lines.append(img_text)

        # 进度提示
        if page_num % 10 == 0:
            print(f"Processed page {page_num}/{len(doc)}")

    cleaned = postprocess("\n".join(out_lines))
    Path(txt_path).write_text(cleaned, encoding="utf-8")
    print(f"Saved => {txt_path}")


def ocr_page(page) -> str:
    """Render one PDF page to image and OCR it."""
    zoom = OCR_DPI / 72                 # PyMuPDF 基于 72 dpi，计算缩放因子
    mat  = fitz.Matrix(zoom, zoom)
    pix  = page.get_pixmap(matrix=mat)

    # 控制输出图片大小，避免巨大扫描页
    img_mb = (pix.width * pix.height * 4) / (1024 * 1024)
    if img_mb > MAX_IMG_MB:
        scale = (MAX_IMG_MB * 1024 * 1024 / (pix.width * pix.height * 4)) ** 0.5
        mat = fitz.Matrix(scale, scale)
        pix = page.get_pixmap(matrix=mat)

    # 写临时 PNG 做 OCR
    fd, tmp_path = mkstemp(suffix=".png")
    os.close(fd)
    pix.save(tmp_path)

    img = Image.open(tmp_path)
    text = pytesseract.image_to_string(img, lang=TESSERACT_LANG)
    os.remove(tmp_path)
    return text


# ----------  文本清洗 ----------
def postprocess(raw: str) -> str:
    """Basic cleaning: remove page numbers, merge broken lines, strip noise."""
    # 1) 去掉孤立页码 / 页眉(例: 'Page 123', '第 123 页')
    pattern_page_num = re.compile(r"^\s*(Page|页|第)?.{0,3}\d{1,4}\s*(页|Page)?\s*$")
    lines = [ln for ln in raw.splitlines() if not pattern_page_num.match(ln)]

    # 2) 合并断行：若一行末尾不是句末标点则与下一行连接
    merged = []
    buf = ""
    for ln in lines:
        if not ln.strip():
            # 遇空行输出缓冲区
            merged.append(buf)
            buf = ""
            continue
        buf += ln.strip()
        if re.search(r"[.!?。！？]$", ln):   # 句末标点视为段落结束
            merged.append(buf)
            buf = ""
        else:
            buf += " "                      # 行内继续

    merged.append(buf)
    merged = [p for p in merged if p.strip()]

    # 3) 返回清理结果
    return "\n\n".join(merged)


# ----------  Demo ----------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="PDF ➜ TXT extractor")
    parser.add_argument("pdf", help="Input PDF path")
    parser.add_argument("-o", "--out", help="Output TXT path (default: same name)")
    args = parser.parse_args()

    pdf_file = args.pdf
    txt_file = args.out or Path(pdf_file).with_suffix(".txt")
    pdf_to_txt(pdf_file, txt_file)
```

### 使用示例

```bash
python pdf2txt.py ./books/Wealth_of_Nations_en.pdf
python pdf2txt.py ./books/国富论_中文版.pdf -o ./books/Wealth_of_Nations_zh.txt
```

---

## 2 . EPUB ➜ TXT

### 方案 A：调用 Calibre CLI

```bash
ebook-convert book.epub book.txt
```

> `ebook-convert` 会自动剥掉 HTML 标签并输出纯文本。再用简单脚本对 TXT 运行 `postprocess()` 即可完成清洗。

### 方案 B：纯 Python（使用 ebooklib）

```python

from ebooklib import epub
from bs4 import BeautifulSoup      # pip install beautifulsoup4
from pathlib import Path
import re

def epub_to_txt(epub_path: str, txt_path: str) -> None:
    book  = epub.read_epub(epub_path)
    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))

    paragraphs = []
    for item in items:
        soup = BeautifulSoup(item.get_body_content(), "lxml")
        text = soup.get_text(separator="\n")
        paragraphs.append(text)

    cleaned = postprocess("\n".join(paragraphs))
    Path(txt_path).write_text(cleaned, encoding="utf-8")

# 示例
# epub_to_txt("Sapiens_en.epub", "Sapiens_en.txt")
```

Any other format, then convert to TSV, tabular seperated. TSV can then be converted to huggingface compatibles;


---

## 3 . 脚本集成与批处理建议

1. **准备清单**：手工或爬虫生成 `book_list.csv`  
   | book_id | lang | pdf_path / epub_path | txt_out          |
   | ------- | ---- | -------------------- | ---------------- |
   | 001     | en   | …/Wealth_en.pdf     | …/Wealth_en.txt |
   | 001     | zh   | …/国富论.pdf        | …/Wealth_zh.txt |
2. **批量运行**：写一个 `process_books.py` 循环调用上面两个转换函数。根据文件后缀自动调用 `pdf_to_txt` or `epub_to_txt`。
3. **异常记录**：对 OCR 失败、乱码或读取异常的文件记录日志，稍后人工处理。
4. **多进程并行**：可用 `concurrent.futures.ProcessPoolExecutor` 开启 4‑8 进程并行转换，显著提升速度。若同时跑 OCR，要注意 CPU 负载与内存。

---

## 4 . 额外清洗要点

| 问题                       | 处理方法 (示例正则或脚本思路)                                             |
| -------------------------- | ------------------------------------------------------------------------- |
| 页眉带章节名反复出现       | `re.sub(r"^Chapter \d+.*$", "", text, flags=re.M)`                      |
| 重复空行                   | `re.sub(r"\n{3,}", "\n\n", text)`                                       |
| 英文长单词跨行连字符 `-` | `re.sub(r"-\n([a-zA-Z])", r"\1", text)`                                 |
| OCR 错字 (如 `             | `→`l`)                                                                 |
| BOM / 编码问题             | 确保 `open(..., encoding="utf-8")`，写文件时加 `errors="ignore"` 选项 |

---

这份脚本组合了**文字层提取 + OCR 回退 + 清洗**的完整链路，并用函数划分，方便你在批量爬取完书籍后直接调用。按照同样方式处理英文版和中文版，最终便能得到干净的 `.txt` 文件，为后续的段落对齐和 8k 切分做好准备。祝项目顺利！
