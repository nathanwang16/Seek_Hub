{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epub_to_txt(epub_path: str, txt_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Extract plain text from an EPUB file, run the same post-processing\n",
    "    pipeline, and save as UTF-8 .txt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epub_path : str | Path\n",
    "        Input .epub file.\n",
    "    txt_path  : str | Path\n",
    "        Target .txt file.\n",
    "    \"\"\"\n",
    "    # read the entire EPUB package\n",
    "    book = epub.read_epub(epub_path)\n",
    "\n",
    "    # concatenate all xhtml/html documents in spine order\n",
    "    chunks = []\n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        soup = BeautifulSoup(item.get_content(), \"lxml\")\n",
    "        chunks.append(soup.get_text(separator=\"\\n\"))\n",
    "\n",
    "    raw_text = \"\\n\\n\".join(chunks)\n",
    "\n",
    "    # reuse your existing postprocess() function\n",
    "    cleaned = postprocess(raw_text)\n",
    "\n",
    "    # save\n",
    "    Path(txt_path).write_text(cleaned, encoding=\"utf-8\")\n",
    "    print(f\"Saved => {txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epub post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    对 PDF/EPUB 提取出的原始文本进行清洗，返回 UTF-8 字符串。\n",
    "    步骤：\n",
    "    0) 换行/控制字符标准化\n",
    "    1) 去页码 + 高频页眉页脚\n",
    "    2) 合并断行，保留空行作段落分隔\n",
    "    3) 修复跨行连字符 “hy-\\nphen”\n",
    "    4) 折叠多余空行\n",
    "    5) 可选：标点半/全角转换、繁简体转换等\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 0) 统一换行符 & 去除控制字符                                        #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    txt = (\n",
    "        raw.replace(\"\\r\\n\", \"\\n\")          # CRLF → LF\n",
    "           .replace(\"\\r\", \"\\n\")            # CR → LF\n",
    "           .lstrip(\"\\ufeff\")               # 去 UTF-8 BOM\n",
    "    )\n",
    "    # 删除除 LF/TAB 外的其他 ASCII 控制字符\n",
    "    txt = re.sub(r\"[\\x00-\\x09\\x0B-\\x1F\\x7F]\", \"\", txt)\n",
    "\n",
    "    lines = txt.split(\"\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 1) 去孤立页码 & 高频页眉/页脚                                      #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    pat_page = re.compile(r\"^\\s*(?:Page\\s*)?\\d{1,4}\\s*(?:页|Page)?\\s*$\")\n",
    "    trimmed = [ln.strip() for ln in lines if ln.strip()]\n",
    "    # 统计出现频次，出现率超过阈值且长度 <80 的行视为模板化噪声\n",
    "    common  = {ln for ln, c in Counter(trimmed).items()\n",
    "                     if c > len(lines) * _HEADER_THRESHOLD and len(ln) < 80}\n",
    "\n",
    "    def is_noise(line: str) -> bool:\n",
    "        return pat_page.match(line) or line.strip() in common\n",
    "\n",
    "    content_lines = [ln for ln in lines if not is_noise(ln)]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 2) 合并断行：非句末行与下一行拼接                                  #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    merged, buf = [], \"\"\n",
    "    for ln in content_lines:\n",
    "        if not ln.strip():           # 空行 → 段落边界\n",
    "            if buf:\n",
    "                merged.append(buf)\n",
    "                buf = \"\"\n",
    "            continue\n",
    "\n",
    "        buf += ln.strip()            # 去两端空白再拼\n",
    "        if re.search(_SENT_END + r\"$\", ln):  # 句末标点→刷新缓冲\n",
    "            merged.append(buf)\n",
    "            buf = \"\"\n",
    "        else:\n",
    "            buf += \" \"               # 行内继续，加空格\n",
    "\n",
    "    if buf:\n",
    "        merged.append(buf)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 3) 修复 “hy-\\nphen” 这类跨行连字符                                 #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    text_block = \"\\n\".join(merged)\n",
    "    text_block = re.sub(r\"-\\s*\\n([a-zA-Z])\", r\"\\1\", text_block)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 4) 连续 ≥3 个换行压缩成 1 个空行                                   #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    text_block = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_block)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 5) 可选增强：如需可取消注释                                        #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # # a) 全角书名号《》 → 半角引号\n",
    "    # text_block = text_block.replace(\"《\", \"\\\"\").replace(\"》\", \"\\\"\")\n",
    "    #\n",
    "    # # b) 繁体转简体\n",
    "    # # from opencc import OpenCC\n",
    "    # # text_block = OpenCC(\"t2s\").convert(text_block)\n",
    "\n",
    "    return text_block.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "epub_to_txt(\"北京枪声 (Unknown) (Z-Library).epub\", \"北京枪声 (Unknown) (Z-Library).epub.txt\")\n",
    "txt_path = Path(\"北京枪声 (Unknown) (Z-Library).epub.txt\")\n",
    "print(txt_path.exists())          # True → 文件确实写好了\n",
    "print(txt_path.stat().st_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertalign.aligner import Aligner\n",
    "import nltk, regex\n",
    "nltk.download(\"punkt\")\n",
    "aligner = Aligner(model_name=\"sentence-transformers/LaBSE\")  # 会自动下载模型\n",
    "\n",
    "def split_en(text):                 # NLTK 英文分句\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def split_zh(text):                 # 正则中文分句\n",
    "    return [s for s in regex.split(r'(?<=[。！？])\\s*', text) if s.strip()]\n",
    "\n",
    "def align_sentences(en_sents, zh_sents):\n",
    "    \"\"\"\n",
    "    返回按顺序对应的 [(en_idx, zh_idx)] 列表；Bertalign 内部已做动态规划\n",
    "    \"\"\"\n",
    "    pairs = aligner.align(en_sents, zh_sents, threshold=0.5)\n",
    "    return pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
