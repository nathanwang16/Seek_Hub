1.训练分4轮，每次训练的文本长度由8k-64k-128k-128k
  这是由于普通大模型的注意力机制，导致它只能很好地处理千字规模以内的文本。我们的实验证明，用deepseek去翻译长文本时会出现内容省略的情况
  后期当我们四轮训练结束后，可能需要改变模型底层架构，比如使用稀疏注意力机制
  
2.我们使用云端部署，平台叫做Lambda Labs，训练时需租借24块H100芯片组成的算力集群，费用在每小时24刀左右

3.微调前的准备：
  1.准备数据集
    数据集的种类以文科书籍为主，包括所有的专业教材、小说、科普书以及无法分类的书籍。目前训练的科目不包括数学和物理。数据集同时需要加入少量诗歌
    训练数据基本来源于Z-library。第一轮的数据量在100本书左右。

  2.编写爬虫程序去Zlib上大批量下载。
  如果遭到反爬导致下载失败，需要所有人一起去手动下载。到时候我会开通一个无限制下载的账号。

  3.统一数据格式，把不同类型的文件（pdf/txt/epub等）转化成统一的格式（txt）
  4.对翻译文本进行分段，每段长度8k字
  5.中英翻译配对，校对，再做成数据集的格式
